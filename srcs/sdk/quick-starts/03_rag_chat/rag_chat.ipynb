{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クイックスタート3\n",
    "\n",
    "このノートのクイックスタートは、Microsoft の Azure AI Foundry を使って、RAG を行う AI 推論チャット アプリケーションの開発を行うための学習ノートです。\n",
    "\n",
    "※この内容は 2025年2月時点での仕様等に基づくものです。仕様やドキュメントの変更に伴い、この内容が古くなる可能性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前提条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートの内容を学習するにあたって、以下が必要になります。\n",
    "\n",
    "- Visial Studio Code または このノートブックを実行可能な Jupyter が動作する環境\n",
    "- Azure サブスクリプション\n",
    "- 作成済みの Azure AI Foundry プロジェクトとデプロイしたモデル\n",
    "- Azure AI Foundry プロジェクトに接続済みの Application Insights インスタンス（トレースに利用） \n",
    "- Python 3.8 以降\n",
    "- Python に関する基礎知識\n",
    "- 必要な Python パッケージ（下記のコードでインストールする）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要な Python パッケージのインストール\n",
    "%pip install python-dotenv azure-identity azure-ai-projects azure-ai-inference azure-search-documents azure-core-tracing-opentelemetry azure-monitor-opentelemetry opentelemetry-sdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Foundry で RAG を行う AI 推論チャットアプリを開発する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートでは、Azure AI Foundry SDK を使用して、独自のデータを使って RAG を行うシンプルな推論チャットアプリケーションを開発する方法について説明します。\n",
    "\n",
    "Azure AI Search を使って架空のアウトドア製品のサンプルデータの検索インデックスを事前に登録しておきます。ユーザーからのクエリに対して、検索インデックスを使ってクエリに対する類似度の高いドキュメントを取り出し、取り出したドキュメントをグラウンディングデータとしてクエリと合わせて推論モデルに問い合わせを行って回答を得ます。\n",
    "\n",
    "クエリに対する類似度の高いドキュメントの取り出しを行うために、ベクトル類似検索を行います。事前に検索インデックス登録するデータに対して、埋め込みモデルを使ってベクトル化を行い、ベクトル化したデータも合わせて検索インデックスに登録します。また、ユーザーからのクエリに対しても、同様に、ベクトル化を行って、Azure AI Search を使って検索して、ドキュメントを取り出します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクイック スタートを始める前に、Azure AI Foundry ポータルから、ハブ リソースとプロジェクト リソースを作成して `gpt-4o-mini` モデルをプロジェクトへデプロイしてください。\n",
    "\n",
    "また、検索インデックスの作成で利用する `text-embedding-ada-002` エンベディングモデルも合わせてデプロイしてください。\n",
    "\n",
    "> [Azure AI Foundry プレイグラウンドのクイックスタート](https://learn.microsoft.com/ja-jp/azure/ai-studio/quickstarts/get-started-playground) を完了している場合、必要な作業は完了しています。\n",
    "\n",
    "Visual Studio Code でこのノートを開き、[ドキュメントのこのセクション](https://code.visualstudio.com/docs/python/environments#_creating-environments) を参考にしながら、ワークスペースに Python 仮想環境を作成します。仮想環境を作成する際には、このフォルダにある `requirements.txt` を選択すると、このノートの Python アプリケーションコードの実行に必要なライブラリを Python 仮想環境にインストールすることができます。\n",
    "\n",
    "仮想環境の準備ができたら、このノートにある Python コードをステップごとに実行します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チャットアプリのコードサンプル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. AI プロジェクトに接続します\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルデータをベースにした問い合わせを行う RAG チャットアプリのためのアプリの準備を行います。\n",
    "\n",
    "  - `.env` ファイルの内容を環境変数に読み込みます。\n",
    "  - 現在のユーザーで認証するための Azure 資格情報を取得します。\n",
    "  - `AIProjectClient` の `from_connection_string()` を使って、Azure AI Foundry プロジェクトに接続します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from opentelemetry import trace\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "load_dotenv('.env', override=True)\n",
    "project_connection_string = os.getenv('PROJECT_CONNECTION_STRING')\n",
    "model_name_string = os.getenv('MODEL_NAME')\n",
    "embeddings_model_name_string = os.getenv('EMBEDDINGS_MODEL_NAME')\n",
    "search_index_name = os.environ.get(\"SEARCH_INDEX_NAME\")\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    print(f\"✅ Azure 資格情報の初期化が成功しました.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Azure 資格情報の初期化が以下の理由で失敗しました: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    project = AIProjectClient.from_connection_string(\n",
    "        conn_str=project_connection_string,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(f\"✅ プロジェクトクライアントの初期化が成功しました.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ プロジェクトクライアントの初期化が以下の理由で失敗しました: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. トレースを有効にします\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードでは、チャットアプリケーションのトレースを有効にします。\n",
    "\n",
    "> AI プロジェクトに Application Insights インスタンスを接続していない場合はこのセクションをスキップしてください。\n",
    "\n",
    "- `AIInferenceInstrumentor` を使って、Azure AI 推論 SDK のインストルメンテーションを有効にします。\n",
    "- `AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED` 環境変数を `true` に設定して、プロンプトと応答の内容をキャプチャします。\n",
    "- Azure SDK トレースを有効にするには、`AZURE_SDK_TRACING_IMPLEMENTATION` 環境変数を `opentelemetry` に設定します。\n",
    "  または、以下のコードを追加します。\n",
    "\n",
    "    ```python\n",
    "    from azure.core.settings import settings\n",
    "    settings.tracing_implementation = \"opentelemetry\"\n",
    "    ```\n",
    "    \n",
    "- トレース情報を Azure Application Insights (Azure Monitor) に送信するには、`configure_azure_monitor()` で有効にします。\n",
    "    - ここでは Python 向けの `azure.monitor.opentelemetry` ライブラリを利用して、直接、送信（エクスポート）します。\n",
    "    - AI プロジェクトに接続された Azure Application Insights への接続文字列を取得して、`configure_azure_monitor()`に渡します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.tracing import AIInferenceInstrumentor\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "# Enable Azure SDK tracing with either of two lines:\n",
    "os.environ[\"AZURE_SDK_TRACING_IMPLEMENTATION\"] = \"opentelemetry\"\n",
    "settings.tracing_implementation = \"opentelemetry\"\n",
    "\n",
    "# Enable tracing\n",
    "AIInferenceInstrumentor().instrument()\n",
    "# Enable logging message contents\n",
    "os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "\n",
    "application_insights_connection_string = project.telemetry.get_connection_string()\n",
    "if not application_insights_connection_string:\n",
    "    print(\"❌ Application Insights がこのプロジェクトで有効になっていません.\")\n",
    "    print(\"==> Azure AI Foundry ポータルの \\\"トレース\\\"タブから、Application Inights を有効にしてください.\")\n",
    "else:\n",
    "    configure_azure_monitor(connection_string=application_insights_connection_string)\n",
    "    print(\"✅ Application Insights によるトレースを有効にしました.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 検索インデックスを定義します\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードでは、検索インデックスのデータを入れるためのストア (`SearchIndex`) を定義して作成します。\n",
    "\n",
    "取り扱うデータに合わせてフィールドを定義します。この例では、架空のアウトドア製品の情報を格納したファイル ([`outdoor_products_data.py`](./outdoor_products_data.py)) を使用します。\n",
    "また、ベクトル検索で利用するベクトルデータを格納するフィールド (`contentVector`) も定義します。\n",
    "\n",
    "検索インデックスの作成パラメーターは、[`search_index_config.py`](./search_index_config.py) で定義しています。ここでは詳しくは触れません。詳細については、Azure AI Search のドキュメントを参照ください ([1.セマンティックランク付け])  ([2.ベクトル検索によるランク付け])。\n",
    "\n",
    "[1.セマンティックランク付け]:https://learn.microsoft.com/ja-jp/azure/search/semantic-search-overview\n",
    "\n",
    "[2.ベクトル検索によるランク付け]:https://learn.microsoft.com/ja-jp/azure/search/semantic-search-overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex, SearchField,\n",
    "    SearchableField, SimpleField, SearchFieldDataType,\n",
    ")\n",
    "from search_index_config import semantic_search, vector_search\n",
    "\n",
    "dimensions = 1536  # text-embedding-ada-002\n",
    "if embeddings_model_name_string == \"text-embedding-3-large\":\n",
    "    dimensions = 3072\n",
    "\n",
    "fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "    SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"price\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"category\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"brand\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "    SearchField(\n",
    "        name=\"contentVector\",\n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "        searchable=True,\n",
    "        # Size of the vector created by the text-embedding-ada-002 model.\n",
    "        vector_search_dimensions=dimensions,\n",
    "        vector_search_profile_name=\"myHnswProfile\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "search_index_definitions = SearchIndex(\n",
    "    name=search_index_name,\n",
    "    fields=fields,\n",
    "    semantic_search=semantic_search,\n",
    "    vector_search=vector_search,\n",
    ")\n",
    "\n",
    "print(f\"✅ 検索インデックスの定義が成功しました.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 検索インデックスに格納するドキュメント データを作成します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードでは、先ほど準備した検索インデックスに格納するドキュメント データ作成します。\n",
    "\n",
    "AI 推論 API の埋め込みクライアントを使って、サンプルデータのアイテムごとに、コンテンツ フィールド (`content`) に対するベクトル検索データを作成 (`embed()`) します。\n",
    "\n",
    "AI 推論 API の埋め込みモデルは、`.env` ファイルの `EMBEDDINGS_MODEL_NAME` で指定したモデルを使用します。このサンプルでは `text-embedding-ada-002` モデルも既定値として準備しています。AI プロジェクトには、このモデルを事前にデプロイしておきます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アウトドア製品情報の読み込み\n",
    "from outdoor_products_data import outdoor_products\n",
    "\n",
    "\n",
    "@tracer.start_as_current_span(\"quick_start_03: create_search_index_docs\")\n",
    "def create_search_index_docs():\n",
    "    try:\n",
    "        embed_client = project.inference.get_embeddings_client()\n",
    "        print(\"✅ 埋め込みクライアントを作成しました.\")\n",
    "\n",
    "        search_docs = []\n",
    "        for doc in outdoor_products:\n",
    "            embed_response = embed_client.embed(\n",
    "                model=embeddings_model_name_string,\n",
    "                input=doc[\"content\"],\n",
    "            )\n",
    "            embed_vec = embed_response.data[0].embedding\n",
    "\n",
    "            for item in embed_response.data:\n",
    "                vec = item.embedding\n",
    "                sample_str = f\"[{vec[0]:.4f}, {vec[1]:.4f}, ..., {vec[-2]:.4f}, {vec[-1]:.4f}]\"\n",
    "                print(f\"アイテム: '{doc[\"content\"]}':\\n\" \\\n",
    "                      f\"  埋め込みの長さ={len(vec)}\\n\" \\\n",
    "                      f\"  ベクトルデータ: {sample_str}\\n\")\n",
    "\n",
    "            search_docs.append(\n",
    "              {\n",
    "                  \"id\": doc[\"id\"],\n",
    "                  \"title\": doc[\"name\"],\n",
    "                  \"price\": str(doc[\"price\"]) + \"ドル\",\n",
    "                  \"category\": doc[\"category\"],\n",
    "                  \"brand\": doc[\"brand\"],\n",
    "                  \"content\": doc[\"content\"],\n",
    "                  \"contentVector\": embed_vec,\n",
    "              }\n",
    "            )\n",
    "\n",
    "        print(\"✅ アウトドア製品のサンプルデータに対する検索インデックスデータが準備できました.\")\n",
    "        return search_docs\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ 埋め込みクライアントの処理が以下の理由で失敗しました:\", str(e))\n",
    "        return []\n",
    "\n",
    "search_index_documents = create_search_index_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 検索インデックスを作成し、ドキュメントをアップロードします"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードを実行して、AI プロジェクトに接続された Azure AI Search の接続情報を取得します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import ConnectionType\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "\n",
    "@tracer.start_as_current_span(\"quick_start_03: get_search_connection\")\n",
    "def get_search_connection():\n",
    "    # Get the default search connection, with credentials\n",
    "    search_connection = project.connections.get_default(\n",
    "        connection_type=ConnectionType.AZURE_AI_SEARCH,\n",
    "        include_credentials=True\n",
    "    )\n",
    "    if not search_connection:\n",
    "        print(\"❌ 既定の Azure AI Search インスタンスへの接続が見つかりませんでした.\")\n",
    "    else:\n",
    "        print(\"✅ 既定の Azure AI Search インスタンスへの接続を準備しました.\")\n",
    "    return search_connection\n",
    "\n",
    "search_connection = get_search_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードを実行して、`SearchIndexClient` クライアントを初期化して、定義した検索インデックスを Azure AI Search のインスタンス上に作成します。\n",
    "`SEARCH_INDEX_NAME` 環境変数で定義された名前で、検索インデックスを作成します。\n",
    "\n",
    "> すでに同じ名前の検索インデックスが存在する場合は、既存の検索インデックスを削除します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "\n",
    "\n",
    "@tracer.start_as_current_span(\"quick_start_03: create_search_index\")\n",
    "def create_search_index(search_connection, search_index_definitions):\n",
    "    try:\n",
    "        index_client = SearchIndexClient(\n",
    "            endpoint=search_connection.endpoint_url,\n",
    "            credential=AzureKeyCredential(key=search_connection.key),\n",
    "        )\n",
    "        print(\"✅ SearchIndexClient を作成しました.\")\n",
    "        try:\n",
    "            index_definition = index_client.get_index(search_index_name)\n",
    "            index_client.delete_index(search_index_name)\n",
    "            print(f\"🗑️ 既存のインデックス ({search_index_name}) が見つかりましたので、削除しました.\")\n",
    "        # pylint: disable=broad-exception-caught\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        index_client.create_index(index=search_index_definitions)\n",
    "        print(f\"✅ インデックス {search_index_name}' を作成しました.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ SearchIndexClient の作成が以下の理由で失敗しました:\", str(e))\n",
    "\n",
    "create_search_index(search_connection, search_index_definitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードを実行して、`SearchClient` クライアントを初期化して、作成した検索インデックスにドキュメント データをアップロードします。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "\n",
    "\n",
    "@tracer.start_as_current_span(\"quick_start_03: upload_search_index_docs\")\n",
    "def upload_search_index_docs(search_connection, search_index_documents):\n",
    "    try:\n",
    "        search_client = SearchClient(\n",
    "          endpoint=search_connection.endpoint_url,\n",
    "          index_name=search_index_name,\n",
    "          credential=AzureKeyCredential(search_connection.key)\n",
    "        )\n",
    "        print(\"✅ SearchClient クライアントを作成しました.\")\n",
    "        result = search_client.upload_documents(documents=search_index_documents)\n",
    "        print(f\"➕ {len(search_index_documents)} 個のドキュメントをインデックス {search_index_name}' にアップロードしました.\")\n",
    "    except Exception as e:\n",
    "        print(\"❌ SearchClient クライアントの作成が以下の理由で失敗しました:\", str(e))\n",
    "\n",
    "\n",
    "upload_search_index_docs(search_connection, search_index_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. RAG を使って登録したドキュメントでユーザーの質問へ回答します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードを実行して、RAG を行うチャット関数を定義します。\n",
    "\n",
    "- ユーザーの質問に対して、埋め込みモデルでベクトルデータを作成します\n",
    "- ユーザーの質問のベクトルデータに対してベクトル類似検索を使って、インデックス検索に登録したドキュメントデータに対して検索を行います\n",
    "- 検索結果のドキュメントを含めたシステムメッセージを作成します\n",
    "- システムメッセージとユーザーからの質問で、推論モデルに問い合わせを行います "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "\n",
    "@tracer.start_as_current_span(\"quick_start_03: rag_chat\")\n",
    "def rag_chat(query: str) -> str:\n",
    "    try:\n",
    "        # 1) ユーザーのクエリの埋め込みの取得\n",
    "        embed_client = project.inference.get_embeddings_client()\n",
    "        print(\"✅ 埋め込みクライアントを作成しました.\")\n",
    "        embedding = embed_client.embed(\n",
    "            input=[query],\n",
    "            model=embeddings_model_name_string,\n",
    "        ).data[0].embedding\n",
    "\n",
    "        # 2) ベクトル類似検索で検索を行う\n",
    "        search_client = SearchClient(\n",
    "          endpoint=search_connection.endpoint_url,\n",
    "          index_name=search_index_name,\n",
    "          credential=AzureKeyCredential(search_connection.key)\n",
    "        )\n",
    "        print(\"✅ SearchClient クライアントを作成しました.\")\n",
    "        vector_query = VectorizedQuery(vector=embedding, k_nearest_neighbors=50, fields=\"contentVector\")\n",
    "        results = search_client.search(\n",
    "            search_text=None,\n",
    "            vector_queries= [vector_query],\n",
    "            select=[\"title\", \"category\", \"brand\", \"price\", \"content\"],\n",
    "            top=3,  # トップ3のドキュメントを取得します\n",
    "        )\n",
    "        top_docs_content = []\n",
    "        for r in results:\n",
    "            # Each doc is a SearchResult object with doc: dict.\n",
    "            top_docs_content.append(\n",
    "                f\"\"\"\n",
    "                製品名: {r[\"title\"]}\n",
    "                カテゴリ：{r[\"category\"]}\n",
    "                ブランド名：{r[\"brand\"]}\n",
    "                価格：{r[\"price\"]}\n",
    "                製品の概要：\n",
    "                {r[\"content\"]}\n",
    "\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "        # 4) 取得したドキュメントを使ってシステムプロンプトを作成する\n",
    "        # ここでは、ユーザーからのクエリに対する対応の手順を示したシステムメッセージを推論モデルに渡します\n",
    "        system_text = (\n",
    "            \"# 手順\\n\"\n",
    "            \"- あなたは、アウトドア・キャンプ用品や衣類に関するクエリでユーザーを支援する AI アシスタントです。\\n\"\n",
    "            \"- 質問がアウトドア・キャンプ用品や衣類に関連しているが漠然としている場合は、\"\n",
    "            \"ドキュメントを参照して回答するのではなく、明確にする質問をしてください。\\n\"\n",
    "            \"- 質問が一般的な場合 (たとえば、「それ」や「彼ら」が使用されている場合)、\"\n",
    "            \"ユーザーに質問している製品を指定するように依頼してください。\\n\"\n",
    "            \"- 質問がアウトドア・キャンプ用品や衣類に関連している場合でも、該当する製品の情報がない場合は、その旨を伝えてください。\\n\"\n",
    "            \"- 質問がアウトドア・キャンプ用品や衣類に関連していない場合は、\"\n",
    "            \"「申し訳ありませんが、アウトドア・キャンプ用品や衣類に関するクエリにしか回答できません。では、どのようにお手伝いすればよいでしょうか？」とだけ言ってください。\\n\"\n",
    "            \"- 答えをでっち上げないでください。\\n\"\n",
    "            \"- 以下のコンテキストを使用して、アウトドア/キャンプ用品と衣類に関する質問に、できるだけ完全に、正確に、簡潔に回答してください。\\n\"\n",
    "            \"\\n\"\n",
    "            \"# ドキュメント\\n\"\n",
    "            \"\\n\"\n",
    "            + \"\\n\".join(top_docs_content)\n",
    "        )\n",
    "\n",
    "        # 4) ドキュメントデータを含むシステムプロンプトでチャットを行う\n",
    "        with project.inference.get_chat_completions_client() as rag_chat_client:\n",
    "            response = rag_chat_client.complete(\n",
    "                model=model_name_string,\n",
    "                messages=[\n",
    "                    SystemMessage(content=system_text),\n",
    "                    UserMessage(content=query),\n",
    "                ]\n",
    "            )\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"❌ RAG チャットの処理が以下の理由で失敗しました:\", str(e))\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. ユーザーの質問で RAG チャットを行います"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほど作成した RAG チャット関数を、様々なユーザーの質問を使って呼び出し、結果を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"TrekReady ハイキングブーツの価格はいくらですか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"TrekReady ハイキングブーツの特徴を教えてください\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"キャンプ用ストーブの価格はいくらですか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"キャンプ用ストーブの中で一番のおすすめを教えてもらえますか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"TrailMaster X4 テントの材質を教えてください。このテントは雨に強いですか？\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"週末に6人でキャンプに行く予定です。新しいテントを探しています。おすすめを教えていただけますか?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"キャンプ用のバックパックを探しています。おすすめを教えていただけますか?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"日本の首都はどこですか?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    response = rag_chat(query=query)\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
