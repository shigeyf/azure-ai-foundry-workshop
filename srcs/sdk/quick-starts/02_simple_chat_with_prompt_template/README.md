# Quick Start 2: Developing a Simple AI Inference Chat Application with Azure AI Foundry (Continued)

[English](./README.md) | [日本語](./README.ja.md)

In the following notes, we will explain how to develop a simple chat application using the Azure AI Foundry SDK. You will call an inference model (LLM) with user-defined prompts and evaluate the output from the inference model (LLM).

This quick start is an extension of [Quick Start 1](../01_simple_inference_chat/) with the following additional points:

- Use `PromptTemplate` to prepare a standard template
- Enable tracing of the AI inference API

[Jupyter Notebook (日本語)](./chatprompt.ja.ipynb)
