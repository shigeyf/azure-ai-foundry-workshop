{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クイックスタート1\n",
    "\n",
    "このノートのクイックスタートは、2025年2月時点での Microsoft の Azure AI Foundry の[ドキュメント](https://learn.microsoft.com/ja-jp/azure/ai-studio/quickstarts/get-started-code)に記載されている内容になります。\n",
    "\n",
    "※仕様やドキュメントの変更に伴い、この内容が古くなる可能性があります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前提条件\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートの内容を学習するにあたって、以下が必要になります。\n",
    "\n",
    "- Visial Studio Code または このノートブックを実行可能な Jupyter が動作する環境\n",
    "- Azure サブスクリプション\n",
    "- 作成済みの Azure AI Foundry プロジェクトとデプロイしたモデル\n",
    "- Python 3.8 以降\n",
    "- Python に関する基礎知識\n",
    "- 必要な Python パッケージ（下記のコードでインストールする）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要な Python パッケージのインストール\n",
    "%pip install python-dotenv azure-identity azure-ai-projects azure-ai-inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Foundry で シンプルな AI 推論チャットアプリを構築する\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このノートでは、Azure AI Foundry SDK を使用して、シンプルチャットアプリケーションを開発する方法について説明します。\n",
    "Python アプリケーションのコードの一部として定義したプロンプトで 推論モデル（LLM）を呼び出して、推論モデル（LLM）からの出力を評価します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 準備\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このクイック スタートを始める前に、Azure AI Foundry ポータルから、ハブ リソースとプロジェクト リソースを作成して `gpt-4o-mini` モデルをプロジェクトへデプロイしてください。\n",
    "\n",
    "> [Azure AI Foundry プレイグラウンドのクイックスタート](https://learn.microsoft.com/ja-jp/azure/ai-studio/quickstarts/get-started-playground) を完了している場合、必要な作業は完了しています。\n",
    "\n",
    "Visual Studio Code でこのノートを開き、[ドキュメントのこのセクション](https://code.visualstudio.com/docs/python/environments#_creating-environments) を参考にしながら、ワークスペースに Python 仮想環境を作成します。仮想環境を作成する際には、このフォルダにある `requirements.txt` を選択すると、このノートの Python アプリケーションコードの実行に必要なライブラリを Python 仮想環境にインストールすることができます。\n",
    "\n",
    "仮想環境の準備ができたら、このノートにある Python コードをステップごとに実行します。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### チャットアプリのコードサンプル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Azure AI Project の Python ライブラリを読み込みます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 環境変数ファイル (`.env`) から 設定情報を読み込みます\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure AI Foundry プロジェクトの接続文字列と推論モデルの名前の文字列を `.env` ファイルから読み込みます。\n",
    "\n",
    "> `.env` ファイルがない場合は `_env` ファイルをテンプレートとして用意していますので、このファイルをコピーして `.env` ファイルを作成してください。\n",
    "\n",
    "`.env` ファイルには、以下の \"**キー** = **値**\" のペアを記述します。\n",
    "\n",
    "- プロジェクトの接続文字列：PROJECT_CONNECTION_STRING=\"**<AI Foundry プロジェクトの接続文字列>**\"\n",
    "- 推論モデルの名前：MODEL_NAME=\"**<デプロイしたLLMモデル名>**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv('.env', override=True)\n",
    "project_connection_string = os.getenv('PROJECT_CONNECTION_STRING')\n",
    "model_name_string = os.getenv('MODEL_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Azure AI Foundry プロジェクトに接続します (プロジェクト クライアントを作成します)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AIProjectClient` のクラスファクトリメソッド `from_connection_string` を使って Azure AI Foundry プロジェクトのクライアントを作成して、プロジェクトに接続します。\n",
    "`from_connection_string` では、(`.env` ファイルに記載した) 環境変数 `PROJECT_CONNECTION_STRING` で指定したプロジェクト接続名を使用します。\n",
    "\n",
    "Azure AI Foundry プロジェクトへの接続のためには、Microsoft Entra ID のユーザーと Azure CLI (`az login`) を使って、事前に Azure へのログイン認証を行っておく必要があります。\n",
    "\n",
    "`AIProjectClient` の同期クライアントを作成するには、次の手順を実行します：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    print(f\"✅ Azure 資格情報の初期化が成功しました.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Azure 資格情報の初期化が以下の理由で失敗しました: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    project = AIProjectClient.from_connection_string(\n",
    "        conn_str=project_connection_string,\n",
    "        credential=credential\n",
    "    )\n",
    "    print(f\"✅ プロジェクトクライアントの初期化が成功しました.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ プロジェクトクライアントの初期化が以下の理由で失敗しました: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. チャット補完クライアントを作成します\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プロジェクト クライアントを使って、プロジェクト内の推論モデルの AI サービスに接続します。\n",
    "\n",
    "この例では、`get_chat_completions_client()` を使って、チャット補完クライアント `chat` (`ChatCompletionsClient` クラスのインスタンス) を作成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    chat = project.inference.get_chat_completions_client()\n",
    "    print(f\"✅ チャットクライアントの初期化が成功しました.\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ チャットクライアントの初期化が以下の理由で失敗しました: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. チャット補完クライアントを使って問い合わせを行います\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "システムプロンプトと問い合わせメッセージをセットして、\n",
    "作成したチャット補完クライアント (`chat`) を使って、推論モデルからの回答を取得 (`complete()`) します。\n",
    "\n",
    "- システムプロンプトと問い合わせメッセージを組み合わせてメッセージとします\n",
    "- 指定した推論モデルに問い合わせを行います\n",
    "\n",
    "システムプロンプトを変更して、推論モデルからの回答を評価してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_message = \"あなたは 2350 年のテクノ パンク ロッカーのように話す AI アシスタントです。クールだけどクール過ぎない。わかりましたか??\"\n",
    "user_query_message = \"税金の手続きを手伝ってくれませんか？私はフリーランサーです。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_message = \"あなたは税務署の職員で、税金に詳しいプロフェッショナルです。質問に対して手続きを細かく説明してください。わかりましたか??\"\n",
    "user_query_message = \"税金の手続きを手伝ってくれませんか？私はフリーランサーです。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.inference.models import SystemMessage, UserMessage\n",
    "\n",
    "response = chat.complete(\n",
    "    model=model_name_string,\n",
    "    messages=[\n",
    "        SystemMessage(\n",
    "            content=system_prompt_message,\n",
    "        ),\n",
    "        UserMessage(\n",
    "            content=user_query_message,\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
